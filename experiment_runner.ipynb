{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DavidAttenborough.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gfvQQ2IqjjX",
        "colab_type": "text"
      },
      "source": [
        "# David Attenborough Speech Generator\n",
        "\n",
        "Following notebook serves as means to train and test tacotron model producing David Attenborough speech.\n",
        "\n",
        "First, upload the speech data to your Google Drive/DavidAttenboroughData/. Folder structure of the uploaded data is not really important as long as you also upload train, validation and test .txt files consisting of filenames and transcriptions. For instructions on how these files should be formatted, take a look [here](https://github.com/NVIDIA/tacotron2/tree/master/filelists)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adviouYMmrxI",
        "colab_type": "code",
        "outputId": "dba952b8-7e56-4fbb-a960-fbb5e8dbf8a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "dataset_filepath = 'C:\\\\Users\\\\Jonathan\\\\Documents\\\\Other shit\\\\DavidAttenboroughSpeechGen\\\\data\\\\SirDavid'\n",
        "project_root = '/content'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jl0_7cvD6GYG",
        "colab_type": "text"
      },
      "source": [
        "## Initialize the environment\n",
        "\n",
        "The following lines run Linux commands necessary to clone tacotron2 repository and install dependencies.\n",
        "\n",
        "Afterwards sort out the imports\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eor7zO5Memz",
        "colab_type": "code",
        "outputId": "21fdc8da-ada3-4419-fe8d-70e1c24cbd6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!git clone https://github.com/cpuimage/Tacotron-2.git\n",
        "%cd '/content/Tacotron-2'\n",
        "!pip install falcon inflect audioread Unidecode librosa matplotlib numpy scipy tqdm keras msgpack msgpack_numpy tensorflow_gpu==1.15\n",
        "!apt install -y libasound-dev portaudio19-dev libportaudio2 libportaudiocpp0 ffmpeg libav-tools\n",
        "%cd '..'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Tacotron-2'...\n",
            "remote: Enumerating objects: 43, done.\u001b[K\n",
            "remote: Counting objects: 100% (43/43), done.\u001b[K\n",
            "remote: Compressing objects: 100% (39/39), done.\u001b[K\n",
            "remote: Total 43 (delta 6), reused 35 (delta 1), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (43/43), done.\n",
            "/content\n",
            "Requirement already satisfied: falcon in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "Requirement already satisfied: audioread in /usr/local/lib/python3.6/dist-packages (2.1.8)\n",
            "Requirement already satisfied: Unidecode in /usr/local/lib/python3.6/dist-packages (1.1.1)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.6/dist-packages (0.6.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.38.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
            "Requirement already satisfied: msgpack_numpy in /usr/local/lib/python3.6/dist-packages (0.4.4.3)\n",
            "Requirement already satisfied: tensorflow_gpu==1.15 in /usr/local/lib/python3.6/dist-packages (1.15.0)\n",
            "Requirement already satisfied: resampy>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.2.2)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.38.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.47.0)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.22.2.post1)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.12.0)\n",
            "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.14.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.15) (1.15.1)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.15) (0.2.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.15) (0.8.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.15) (0.9.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.15) (3.2.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.15) (3.10.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.15) (1.1.0)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.15) (1.15.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.15) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.15) (1.12.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.15) (1.27.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.15) (0.34.2)\n",
            "Requirement already satisfied: llvmlite>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.38.0->librosa) (0.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba>=0.38.0->librosa) (46.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow_gpu==1.15) (3.2.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow_gpu==1.15) (1.0.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Note, selecting 'libasound2-dev' instead of 'libasound-dev'\n",
            "Package libav-tools is not available, but is referred to by another package.\n",
            "This may mean that the package is missing, has been obsoleted, or\n",
            "is only available from another source\n",
            "However the following packages replace it:\n",
            "  ffmpeg\n",
            "\n",
            "E: Package 'libav-tools' has no installation candidate\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGB2N1du752e",
        "colab_type": "code",
        "outputId": "4d33ff6f-837e-4e17-a18a-1b0bcb1591e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        }
      },
      "source": [
        "import os\n",
        "os.chdir(os.path.join(ipynb_root, 'Tacotron-2'))\n",
        "import argparse\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import infolog\n",
        "from hparams import hparams\n",
        "from preprocess import write_metadata\n",
        "from datasets.preprocessor import _process_utterance\n",
        "from train import train, prepare_run"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will switch to TensorFlow 2.x on the 27th of March, 2020.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now\n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /content/Tacotron-2/models/modules.py:41: The name tf.layers.Layer is deprecated. Please use tf.compat.v1.layers.Layer instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/Tacotron-2/models/modules.py:139: The name tf.nn.rnn_cell.RNNCell is deprecated. Please use tf.compat.v1.nn.rnn_cell.RNNCell instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqJxfaDoYUZW",
        "colab_type": "text"
      },
      "source": [
        "## Handle preprocessing\n",
        "\n",
        "First, correct the wavs paths in .txt target files.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNgwDsQdYdPM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this function is unique to current file structure. it probably won't work in general.\n",
        "def change_wav_paths(txt_filepath):\n",
        "  f = open(txt_filepath, \"r\")\n",
        "  lines = f.read().split(\"\\n\")\n",
        "  f.close()\n",
        "  new_file = \"\"\n",
        "  for line in lines:\n",
        "    if len(line) > 0:\n",
        "      line = line.split('|')\n",
        "      transcript = line[1]\n",
        "      # ... because of the next line.\n",
        "      audio_filename = line[0].split('\\\\')[-2:]\n",
        "      audio_filepath = os.path.join(dataset_filepath, *audio_filename)\n",
        "      new_file += audio_filepath + '|' + transcript + \"\\n\"\n",
        "  f = open(txt_filepath, 'w')\n",
        "  f.write(new_file)\n",
        "  f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUMK7n-UePvS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "change_wav_paths(os.path.join(dataset_filepath, 'sir_david_train.txt'))\n",
        "change_wav_paths(os.path.join(dataset_filepath, 'sir_david_val.txt'))\n",
        "change_wav_paths(os.path.join(dataset_filepath, 'sir_david_test.txt'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rq3XackHeCdF",
        "colab_type": "text"
      },
      "source": [
        "Get your dataset and modify datasets/preprocessor.py -> \"build_from_path\" function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tL6GcGlLenKL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_from_path(hparams, input_dir, output_dir):\n",
        "  if isinstance(input_dir, str):\n",
        "    input_dir = [input_dir]\n",
        "\n",
        "  futures = []\n",
        "  i = 1\n",
        "  for _input_dir in input_dir:\n",
        "    f = open(_input_dir, 'r')\n",
        "    lines = f.readlines()\n",
        "    f.close()\n",
        "\n",
        "    for line in lines:\n",
        "      wav_path, seq_text = line.strip().split('|')\n",
        "      futures.append(_process_utterance(output_dir, i, wav_path, seq_text, hparams))\n",
        "      i += 1\n",
        "\n",
        "  return futures"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgM0nnmraINj",
        "colab_type": "code",
        "outputId": "43c731ba-af75-485f-b1ea-698af85961c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "input_dir = [os.path.join(dataset_filepath, txt) for txt in ['sir_david_train.txt', 'sir_david_val.txt', 'sir_david_test.txt']]\n",
        "output_dir = os.path.join(ipynb_root, \"data\")\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "metadata = build_from_path(hparams, input_dir, output_dir)\n",
        "write_metadata(metadata, output_dir)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Write 4624 utterances, 1306762.0 mel frames, 261352400.0 audio timesteps, (4.54 hours)\n",
            "Max input length (text chars): 83\n",
            "Max mel frames length: 585\n",
            "Max audio timesteps length: 117000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zn_9Z5PVpeIz",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb-LbrRHzGmP",
        "colab_type": "text"
      },
      "source": [
        "Train your Tacotron model. Yields the logs-Tacotron2 folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lslGCXRGqPFo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main(argv):\n",
        "  parser = argparse.ArgumentParser()\n",
        "  # these arguments prevent jupyter notebook from throwing an error.\n",
        "  parser.add_argument('-f')\n",
        "  # these arguments are actually used for training. Change the default parameter if you want to change them.\n",
        "  parser.add_argument('--base_dir', default=ipynb_root)\n",
        "  parser.add_argument('--hparams', default='', help='Hyperparameter overrides as a comma-separated list of name=value pairs')\n",
        "  parser.add_argument('--train_input', default=os.path.join(ipynb_root, 'data/train.txt'))\n",
        "  parser.add_argument('--name', help='Name of logging directory.')\n",
        "  parser.add_argument('--model', default='SirDavid_first_run')\n",
        "  parser.add_argument('--output_dir', default=os.path.join(ipynb_root, 'output'), help='folder to contain synthesized mel spectrograms')\n",
        "  parser.add_argument('--restore', type=bool, default=True, help='Set this to False to do a fresh training')\n",
        "  parser.add_argument('--summary_interval', type=int, default=250, help='Steps between running summary ops')\n",
        "  parser.add_argument('--embedding_interval', type=int, default=5000, help='Steps between updating embeddings projection visualization')\n",
        "  parser.add_argument('--checkpoint_interval', type=int, default=2500, help='Steps between writing checkpoints')\n",
        "  parser.add_argument('--eval_interval', type=int, default=5000, help='Steps between eval on test data')\n",
        "  parser.add_argument('--train_steps', type=int, default=100000, help='total number of tacotron2 training steps')\n",
        "  parser.add_argument('--tf_log_level', type=int, default=3, help='Tensorflow C++ log level.')\n",
        "  parser.add_argument('--slack_url', default=None, help='slack webhook notification destination link')\n",
        "  parser.add_argument('--gpu_devices', default='0', help='Set the gpu the model should run on.(eg:hvd,0,1,2...)')\n",
        "  # args = parser.parse_args(argv[1:])\n",
        "  args = parser.parse_args()\n",
        "  use_hvd = args.gpu_devices == 'hvd'\n",
        "  if not use_hvd:\n",
        "      os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu_devices\n",
        "  log_dir, _hparams = prepare_run(args)\n",
        "  train(log_dir, args, _hparams, use_hvd=use_hvd)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLWjcKsjuNXr",
        "colab_type": "code",
        "outputId": "514bb781-90f0-43cf-c653-904a71eec0ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# run training\n",
        "tf.app.run(main=main)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checkpoint path: /content/logs-SirDavid_first_run/pretrained/model.ckpt\n",
            "Loading training data from: /content/data/train.txt\n",
            "Using model: SirDavid_first_run\n",
            "Hyperparameters:\n",
            "  adam_beta1: 0.9\n",
            "  adam_beta2: 0.999\n",
            "  adam_epsilon: 1e-06\n",
            "  allow_clipping_in_normalization: True\n",
            "  attention_dim: 128\n",
            "  batch_size: 32\n",
            "  clip_gradients: True\n",
            "  clip_mels_length: True\n",
            "  clip_outputs: True\n",
            "  cross_entropy_pos_weight: 20\n",
            "  data_random_state: 1234\n",
            "  decay_learning_rate: True\n",
            "  decay_rate: 0.5\n",
            "  decay_steps: 50000\n",
            "  decoder_layers: 2\n",
            "  decoder_lstm_units: 512\n",
            "  dropout_rate: 0.5\n",
            "  embedding_dim: 256\n",
            "  enc_conv_channels: 128\n",
            "  enc_conv_kernel_size: (5,)\n",
            "  enc_conv_num_layers: 3\n",
            "  encoder_lstm_units: 128\n",
            "  final_learning_rate: 1e-05\n",
            "  fine_tuning: False\n",
            "  frame_shift_ms: None\n",
            "  griffin_lim_iters: 60\n",
            "  hop_size: 200\n",
            "  initial_learning_rate: 0.001\n",
            "  lower_bound_decay: 0.1\n",
            "  magnitude_power: 2.0\n",
            "  mask_decoder: False\n",
            "  mask_encoder: True\n",
            "  max_abs_value: 4.0\n",
            "  max_iters: 2000\n",
            "  max_mel_frames: 1000\n",
            "  min_level_db: -100\n",
            "  n_fft: 1024\n",
            "  natural_eval: False\n",
            "  num_freq: 513\n",
            "  num_mels: 80\n",
            "  outputs_per_step: 3\n",
            "  pad_sides: 1\n",
            "  post_net_cbhg_out_units: 128\n",
            "  post_net_conv_channels: 128\n",
            "  post_net_max_filter_width: 8\n",
            "  post_net_num_highway: 4\n",
            "  post_net_projection1_out_channels: 256\n",
            "  post_net_projection2_out_channels: 80\n",
            "  postnet_channels: 128\n",
            "  postnet_kernel_size: (5,)\n",
            "  postnet_num_layers: 5\n",
            "  power: 1.5\n",
            "  predict_linear: True\n",
            "  preemphasis: 0.97\n",
            "  preemphasize: True\n",
            "  prenet_layers: [256, 128]\n",
            "  random_seed: 5339\n",
            "  ref_level_db: 20\n",
            "  reg_weight: 1e-07\n",
            "  rescale: True\n",
            "  rescaling_max: 0.999\n",
            "  sample_rate: 16000\n",
            "  scale_regularization: False\n",
            "  signal_normalization: True\n",
            "  start_decay: 50000\n",
            "  stop_at_any: True\n",
            "  swap_with_cpu: False\n",
            "  symmetric_mels: True\n",
            "  synthesis_batch_size: 1\n",
            "  teacher_forcing_decay_alpha: None\n",
            "  teacher_forcing_decay_steps: 280000\n",
            "  teacher_forcing_final_ratio: 0.0\n",
            "  teacher_forcing_init_ratio: 1.0\n",
            "  teacher_forcing_mode: constant\n",
            "  teacher_forcing_ratio: 1.0\n",
            "  teacher_forcing_start_decay: 10000\n",
            "  test_batches: None\n",
            "  test_size: 0.05\n",
            "  trim_fft_size: 512\n",
            "  trim_hop_size: 128\n",
            "  trim_silence: True\n",
            "  trim_top_db: 23\n",
            "  win_size: 800\n",
            "  zoneout_rate: 0.1\n",
            "WARNING:tensorflow:From /content/Tacotron-2/train.py:152: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0329 21:44:44.606408 139754210260864 module_wrapper.py:139] From /content/Tacotron-2/train.py:152: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/Tacotron-2/train.py:156: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0329 21:44:44.608798 139754210260864 module_wrapper.py:139] From /content/Tacotron-2/train.py:156: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loaded metadata for 4624 examples (4.54 hours)\n",
            "WARNING:tensorflow:From /content/Tacotron-2/feeder.py:80: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0329 21:44:44.644646 139754210260864 module_wrapper.py:139] From /content/Tacotron-2/feeder.py:80: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/Tacotron-2/feeder.py:89: The name tf.FIFOQueue is deprecated. Please use tf.queue.FIFOQueue instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0329 21:44:44.651859 139754210260864 module_wrapper.py:139] From /content/Tacotron-2/feeder.py:89: The name tf.FIFOQueue is deprecated. Please use tf.queue.FIFOQueue instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/Tacotron-2/train.py:80: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0329 21:44:44.663042 139754210260864 module_wrapper.py:139] From /content/Tacotron-2/train.py:80: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/Tacotron-2/models/tacotron2.py:64: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0329 21:44:44.670231 139754210260864 module_wrapper.py:139] From /content/Tacotron-2/models/tacotron2.py:64: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/Tacotron-2/models/modules.py:21: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0329 21:44:44.683058 139754210260864 deprecation.py:323] From /content/Tacotron-2/models/modules.py:21: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/Tacotron-2/models/modules.py:472: The name tf.layers.Conv1D is deprecated. Please use tf.compat.v1.layers.Conv1D instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0329 21:44:44.688141 139754210260864 module_wrapper.py:139] From /content/Tacotron-2/models/modules.py:472: The name tf.layers.Conv1D is deprecated. Please use tf.compat.v1.layers.Conv1D instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/Tacotron-2/models/modules.py:480: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0329 21:44:44.788205 139754210260864 deprecation.py:323] From /content/Tacotron-2/models/modules.py:480: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/normalization.py:327: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0329 21:44:44.790004 139754210260864 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/normalization.py:327: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/Tacotron-2/models/modules.py:486: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0329 21:44:44.823749 139754210260864 deprecation.py:323] From /content/Tacotron-2/models/modules.py:486: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/Tacotron-2/models/modules.py:278: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0329 21:44:44.929414 139754210260864 deprecation.py:323] From /content/Tacotron-2/models/modules.py:278: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0329 21:44:44.933408 139754210260864 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:958: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0329 21:44:44.998556 139754210260864 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:958: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:962: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0329 21:44:45.008518 139754210260864 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:962: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/Tacotron-2/models/modules.py:194: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0329 21:44:45.026452 139754210260864 deprecation.py:506] From /content/Tacotron-2/models/modules.py:194: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/Tacotron-2/models/modules.py:200: The name tf.nn.rnn_cell.LSTMStateTuple is deprecated. Please use tf.compat.v1.nn.rnn_cell.LSTMStateTuple instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0329 21:44:45.049658 139754210260864 module_wrapper.py:139] From /content/Tacotron-2/models/modules.py:200: The name tf.nn.rnn_cell.LSTMStateTuple is deprecated. Please use tf.compat.v1.nn.rnn_cell.LSTMStateTuple instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0329 21:44:45.051131 139754210260864 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/Tacotron-2/models/modules.py:293: The name tf.layers.Dense is deprecated. Please use tf.compat.v1.layers.Dense instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0329 21:44:45.189402 139754210260864 module_wrapper.py:139] From /content/Tacotron-2/models/modules.py:293: The name tf.layers.Dense is deprecated. Please use tf.compat.v1.layers.Dense instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/Tacotron-2/models/modules.py:337: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0329 21:44:45.229888 139754210260864 deprecation.py:323] From /content/Tacotron-2/models/modules.py:337: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/Tacotron-2/models/modules.py:397: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0329 21:44:45.616943 139754210260864 deprecation.py:323] From /content/Tacotron-2/models/modules.py:397: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/Tacotron-2/models/helpers.py:123: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0329 21:44:45.640972 139754210260864 module_wrapper.py:139] From /content/Tacotron-2/models/helpers.py:123: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/Tacotron-2/models/modules.py:89: The name tf.layers.MaxPooling1D is deprecated. Please use tf.compat.v1.layers.MaxPooling1D instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0329 21:44:46.039305 139754210260864 module_wrapper.py:139] From /content/Tacotron-2/models/modules.py:89: The name tf.layers.MaxPooling1D is deprecated. Please use tf.compat.v1.layers.MaxPooling1D instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/Tacotron-2/models/modules.py:59: The name tf.assert_equal is deprecated. Please use tf.compat.v1.assert_equal instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0329 21:44:46.824102 139754210260864 module_wrapper.py:139] From /content/Tacotron-2/models/modules.py:59: The name tf.assert_equal is deprecated. Please use tf.compat.v1.assert_equal instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/Tacotron-2/models/modules.py:126: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0329 21:44:47.109156 139754210260864 deprecation.py:323] From /content/Tacotron-2/models/modules.py:126: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:565: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0329 21:44:47.169271 139754210260864 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:565: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "initialisation done\n",
            "WARNING:tensorflow:From /content/Tacotron-2/models/tacotron2.py:188: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0329 21:44:47.330418 139754210260864 module_wrapper.py:139] From /content/Tacotron-2/models/tacotron2.py:188: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Initialized Tacotron2 model. Dimensions (? = dynamic shape): \n",
            "  Train mode:               True\n",
            "  Eval mode:                False\n",
            "  GTA mode:                 False\n",
            "  Synthesis mode:           False\n",
            "  Input:                    (?, ?)\n",
            "  embedding:                (?, ?, 256)\n",
            "  enc conv out:             (?, ?, 128)\n",
            "  encoder out:              (?, ?, 256)\n",
            "  decoder out:              (?, ?, 80)\n",
            "  postnet out:             (?, ?, 80)\n",
            "  mel out:                  (?, ?, 80)\n",
            "  linear out:               (?, ?, 513)\n",
            "  <stop_token> out:         (?, ?)\n",
            "  Tacotron2 Parameters       6.657 Million.\n",
            "WARNING:tensorflow:From /content/Tacotron-2/models/tacotron2.py:240: The name tf.losses.mean_squared_error is deprecated. Please use tf.compat.v1.losses.mean_squared_error instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0329 21:44:47.337353 139754210260864 module_wrapper.py:139] From /content/Tacotron-2/models/tacotron2.py:240: The name tf.losses.mean_squared_error is deprecated. Please use tf.compat.v1.losses.mean_squared_error instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/Tacotron-2/models/tacotron2.py:342: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0329 21:44:47.419472 139754210260864 module_wrapper.py:139] From /content/Tacotron-2/models/tacotron2.py:342: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/Tacotron-2/models/tacotron2.py:304: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0329 21:44:47.429895 139754210260864 module_wrapper.py:139] From /content/Tacotron-2/models/tacotron2.py:304: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/Tacotron-2/models/tacotron2.py:321: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0329 21:44:51.934462 139754210260864 module_wrapper.py:139] From /content/Tacotron-2/models/tacotron2.py:321: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/Tacotron-2/models/tacotron2.py:321: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0329 21:44:51.936105 139754210260864 module_wrapper.py:139] From /content/Tacotron-2/models/tacotron2.py:321: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/Tacotron-2/train.py:44: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0329 21:44:52.843688 139754210260864 module_wrapper.py:139] From /content/Tacotron-2/train.py:44: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/Tacotron-2/train.py:46: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0329 21:44:52.848183 139754210260864 module_wrapper.py:139] From /content/Tacotron-2/train.py:46: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/Tacotron-2/train.py:63: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0329 21:44:53.130906 139754210260864 module_wrapper.py:139] From /content/Tacotron-2/train.py:63: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "initialisation done\n",
            "Initialized Tacotron2 model. Dimensions (? = dynamic shape): \n",
            "  Train mode:               False\n",
            "  Eval mode:                True\n",
            "  GTA mode:                 False\n",
            "  Synthesis mode:           False\n",
            "  Input:                    (?, ?)\n",
            "  embedding:                (?, ?, 256)\n",
            "  enc conv out:             (?, ?, 128)\n",
            "  encoder out:              (?, ?, 256)\n",
            "  decoder out:              (?, ?, 80)\n",
            "  postnet out:             (?, ?, 80)\n",
            "  mel out:                  (?, ?, 80)\n",
            "  linear out:               (?, ?, 513)\n",
            "  <stop_token> out:         (?, ?)\n",
            "  Tacotron2 Parameters       6.657 Million.\n",
            "WARNING:tensorflow:From /content/Tacotron-2/train.py:179: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0329 21:44:54.339359 139754210260864 module_wrapper.py:139] From /content/Tacotron-2/train.py:179: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Tacotron2 training set to a maximum of 100000 steps\n",
            "WARNING:tensorflow:From /content/Tacotron-2/train.py:184: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0329 21:44:54.595660 139754210260864 module_wrapper.py:139] From /content/Tacotron-2/train.py:184: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/Tacotron-2/train.py:191: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0329 21:44:54.597322 139754210260864 module_wrapper.py:139] From /content/Tacotron-2/train.py:191: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/Tacotron-2/train.py:193: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0329 21:44:54.738198 139754210260864 module_wrapper.py:139] From /content/Tacotron-2/train.py:193: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading checkpoint /content/logs-SirDavid_first_run/pretrained/model.ckpt-0\n",
            "INFO:tensorflow:Restoring parameters from /content/logs-SirDavid_first_run/pretrained/model.ckpt-0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0329 21:44:56.418976 139754210260864 saver.py:1284] Restoring parameters from /content/logs-SirDavid_first_run/pretrained/model.ckpt-0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/Tacotron-2/train.py:226: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0329 21:44:57.018497 139754210260864 module_wrapper.py:139] From /content/Tacotron-2/train.py:226: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Generated 7 test batches of size 32 in 3.007 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 5.002 sec\n",
            "Step       1 [7.819 sec/step, loss=12.58734, avg_loss=12.58734]\n",
            "Saving Model Character Embeddings visualization..\n",
            "Tacotron2 Character embeddings have been updated on tensorboard!\n",
            "\n",
            "Generated 32 train batches of size 32 in 20.960 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 25.270 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 25.867 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 9.083 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 2.400 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 3.528 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 4.969 sec\n",
            "Step     250 [1.119 sec/step, loss=3.50418, avg_loss=3.66756]\n",
            "Writing summary at step 250\n",
            "\n",
            "Generated 32 train batches of size 32 in 3.950 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.948 sec\n",
            "\n",
            "Saving alignment, Mel-Spectrograms and griffin-lim inverted waveform..\n",
            "Input at step 300: ^coming in to land on threemeter wings takes some practice.~_______________\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.051 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.036 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.931 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.003 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.019 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.914 sec\n",
            "Step     500 [1.085 sec/step, loss=3.14676, avg_loss=3.21738]\n",
            "Writing summary at step 500\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.031 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.929 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.957 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.991 sec\n",
            "Step     629 [1.098 sec/step, loss=3.20299, avg_loss=3.11227]\n",
            "Generated 32 train batches of size 32 in 1.041 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.987 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.046 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.871 sec\n",
            "Step     750 [1.107 sec/step, loss=3.06770, avg_loss=3.06539]\n",
            "Writing summary at step 750\n",
            "Step     756 [1.087 sec/step, loss=3.05790, avg_loss=3.06781]\n",
            "Generated 32 train batches of size 32 in 1.002 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.875 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.031 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.153 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.949 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.893 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.110 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.014 sec\n",
            "Step    1000 [1.084 sec/step, loss=2.98817, avg_loss=2.95975]\n",
            "Writing summary at step 1000\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.985 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.961 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.947 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.917 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.078 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.053 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.081 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.848 sec\n",
            "Step    1250 [1.085 sec/step, loss=3.01349, avg_loss=2.90699]\n",
            "Writing summary at step 1250\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.900 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.882 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.936 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.102 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.959 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.087 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.886 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.056 sec\n",
            "Step    1500 [1.087 sec/step, loss=2.87389, avg_loss=2.85776]\n",
            "Writing summary at step 1500\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.045 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.891 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.177 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.868 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.985 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.920 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.998 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.902 sec\n",
            "Step    1750 [1.104 sec/step, loss=2.86303, avg_loss=2.80769]\n",
            "Writing summary at step 1750\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.089 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.946 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.140 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.084 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.918 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.183 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.046 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.117 sec\n",
            "Step    2000 [1.091 sec/step, loss=2.85745, avg_loss=2.76681]\n",
            "Writing summary at step 2000\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.160 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.044 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.046 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.083 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.968 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.947 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.951 sec\n",
            "Step    2250 [1.087 sec/step, loss=2.50965, avg_loss=2.71929]\n",
            "Writing summary at step 2250\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.953 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.072 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.060 sec\n",
            "Step    2350 [1.082 sec/step, loss=2.74307, avg_loss=2.73538]\n",
            "Generated 32 train batches of size 32 in 1.067 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.091 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.883 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.944 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.049 sec\n",
            "Step    2500 [1.062 sec/step, loss=2.63678, avg_loss=2.69858]\n",
            "Writing summary at step 2500\n",
            "\n",
            "Saving alignment, Mel-Spectrograms and griffin-lim inverted waveform..\n",
            "Input at step 2500: ^but the first to do so are at the highest risk~___________\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.058 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.197 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.903 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.045 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.898 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.931 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.939 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.064 sec\n",
            "Step    2750 [1.082 sec/step, loss=2.63888, avg_loss=2.67791]\n",
            "Writing summary at step 2750\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.056 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.116 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.006 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.052 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.909 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.919 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.973 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.965 sec\n",
            "Step    3000 [1.086 sec/step, loss=2.49487, avg_loss=2.65595]\n",
            "Writing summary at step 3000\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.853 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.941 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.143 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.087 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.954 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.059 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.044 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.866 sec\n",
            "Step    3250 [1.090 sec/step, loss=2.67185, avg_loss=2.63404]\n",
            "Writing summary at step 3250\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.947 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.016 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.134 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.069 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.162 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.970 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.111 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.859 sec\n",
            "Step    3500 [1.092 sec/step, loss=2.40797, avg_loss=2.60804]\n",
            "Writing summary at step 3500\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.096 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.113 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.011 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.970 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.890 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.088 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.109 sec\n",
            "Step    3750 [1.086 sec/step, loss=2.53099, avg_loss=2.59633]\n",
            "Writing summary at step 3750\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.062 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.847 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.033 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.938 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.947 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.957 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.035 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.931 sec\n",
            "Step    4000 [1.080 sec/step, loss=2.37747, avg_loss=2.57697]\n",
            "Writing summary at step 4000\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.121 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.043 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.822 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.924 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.012 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.825 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.128 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.063 sec\n",
            "Step    4250 [1.076 sec/step, loss=2.69402, avg_loss=2.57541]\n",
            "Writing summary at step 4250\n",
            "Step    4261 [1.083 sec/step, loss=2.50780, avg_loss=2.56768]\n",
            "Generated 32 train batches of size 32 in 0.981 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.021 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.045 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.905 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.082 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.036 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.911 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.943 sec\n",
            "Step    4500 [1.083 sec/step, loss=2.50017, avg_loss=2.54973]\n",
            "Writing summary at step 4500\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.019 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.785 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.036 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.078 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.936 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.972 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.898 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.006 sec\n",
            "Step    4750 [1.099 sec/step, loss=2.24758, avg_loss=2.53464]\n",
            "Writing summary at step 4750\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.883 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.056 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.842 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.004 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.084 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.999 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.036 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.850 sec\n",
            "Step    5000 [1.068 sec/step, loss=2.52897, avg_loss=2.50916]\n",
            "Writing summary at step 5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/7 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Running evaluation at step 5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 7/7 [00:11<00:00,  1.63s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saving eval log to /content/logs-SirDavid_first_run/eval-dir..\n",
            "Eval loss for global step 5000: 2.941\n",
            "Writing eval summary!\n",
            "WARNING:tensorflow:From /content/Tacotron-2/train.py:68: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0329 23:17:34.661182 139754210260864 module_wrapper.py:139] From /content/Tacotron-2/train.py:68: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Saving alignment, Mel-Spectrograms and griffin-lim inverted waveform..\n",
            "Input at step 5000: ^we looked back at our own planet.~_______________________\n",
            "\n",
            "Saving Model Character Embeddings visualization..\n",
            "Tacotron2 Character embeddings have been updated on tensorboard!\n",
            "Step    5025 [1.103 sec/step, loss=2.49588, avg_loss=2.50914]\n",
            "Generated 32 train batches of size 32 in 0.942 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.009 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.002 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.200 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.068 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.119 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.096 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.908 sec\n",
            "Step    5250 [1.085 sec/step, loss=2.47804, avg_loss=2.51092]\n",
            "Writing summary at step 5250\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.026 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.171 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.943 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.840 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.948 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.990 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.151 sec\n",
            "Step    5500 [1.090 sec/step, loss=2.48745, avg_loss=2.50192]\n",
            "Writing summary at step 5500\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.000 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.908 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.816 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.986 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.868 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.985 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.941 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.985 sec\n",
            "Step    5750 [1.068 sec/step, loss=2.40321, avg_loss=2.49369]\n",
            "Writing summary at step 5750\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.910 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.819 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.035 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.077 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.078 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.948 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.163 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.068 sec\n",
            "Step    6000 [1.076 sec/step, loss=2.54909, avg_loss=2.47744]\n",
            "Writing summary at step 6000\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.029 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.957 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.941 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.897 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.088 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.004 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.082 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.992 sec\n",
            "Step    6250 [1.067 sec/step, loss=2.47578, avg_loss=2.46678]\n",
            "Writing summary at step 6250\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.122 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.112 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.061 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.944 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.146 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.109 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.184 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.947 sec\n",
            "Step    6500 [1.079 sec/step, loss=2.45578, avg_loss=2.46295]\n",
            "Writing summary at step 6500\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.927 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.066 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.015 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.113 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.977 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.951 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.962 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.993 sec\n",
            "Step    6750 [1.079 sec/step, loss=2.35639, avg_loss=2.43588]\n",
            "Writing summary at step 6750\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.031 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.946 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.951 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.119 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.880 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.933 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.893 sec\n",
            "Step    7000 [1.062 sec/step, loss=2.44003, avg_loss=2.43175]\n",
            "Writing summary at step 7000\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.040 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.884 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.970 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.250 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.045 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.007 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.106 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.100 sec\n",
            "Step    7250 [1.078 sec/step, loss=2.45884, avg_loss=2.41781]\n",
            "Writing summary at step 7250\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.021 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.926 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.034 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.981 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.955 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.063 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.892 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.987 sec\n",
            "Step    7500 [1.075 sec/step, loss=2.44637, avg_loss=2.41123]\n",
            "Writing summary at step 7500\n",
            "\n",
            "Saving alignment, Mel-Spectrograms and griffin-lim inverted waveform..\n",
            "Input at step 7500: ^but, like a toddler having a tantrum, she doesnt want any help at mealtimes.~_____\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.973 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.916 sec\n",
            "Step    7574 [1.079 sec/step, loss=2.24887, avg_loss=2.40889]\n",
            "Generated 32 train batches of size 32 in 0.890 sec\n",
            "Step    7606 [1.088 sec/step, loss=2.11527, avg_loss=2.41179]\n",
            "Generated 32 train batches of size 32 in 0.892 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.037 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.161 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.043 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.071 sec\n",
            "Step    7750 [1.078 sec/step, loss=2.34667, avg_loss=2.39775]\n",
            "Writing summary at step 7750\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.995 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.834 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.771 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.961 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.060 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.935 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.992 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.019 sec\n",
            "Step    8000 [1.076 sec/step, loss=2.46096, avg_loss=2.40045]\n",
            "Writing summary at step 8000\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.124 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.886 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.965 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.800 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.839 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.045 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.979 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.993 sec\n",
            "Step    8250 [1.092 sec/step, loss=2.31039, avg_loss=2.37667]\n",
            "Writing summary at step 8250\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.031 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.034 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.128 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.951 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.934 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.115 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.128 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.021 sec\n",
            "Step    8500 [1.074 sec/step, loss=2.47932, avg_loss=2.37985]\n",
            "Writing summary at step 8500\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.056 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.021 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.920 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.923 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.976 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.109 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.909 sec\n",
            "Step    8750 [1.069 sec/step, loss=2.39382, avg_loss=2.36868]\n",
            "Writing summary at step 8750\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.121 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.078 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.141 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.982 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.978 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.047 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.979 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.915 sec\n",
            "Step    9000 [1.068 sec/step, loss=2.06799, avg_loss=2.35824]\n",
            "Writing summary at step 9000\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.936 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.089 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.009 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.974 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.848 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.964 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.160 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.032 sec\n",
            "Step    9250 [1.088 sec/step, loss=2.42814, avg_loss=2.35403]\n",
            "Writing summary at step 9250\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.049 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.084 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.054 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.015 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.178 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.977 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.129 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.045 sec\n",
            "Step    9500 [1.086 sec/step, loss=2.47978, avg_loss=2.34068]\n",
            "Writing summary at step 9500\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.915 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.066 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.007 sec\n",
            "Step    9614 [1.043 sec/step, loss=2.27984, avg_loss=2.34562]\n",
            "Generated 32 train batches of size 32 in 1.066 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.991 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.943 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.917 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.023 sec\n",
            "Step    9750 [1.070 sec/step, loss=2.39206, avg_loss=2.33938]\n",
            "Writing summary at step 9750\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.915 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.084 sec\n",
            "Step    9837 [1.062 sec/step, loss=2.18234, avg_loss=2.33509]\n",
            "Generated 32 train batches of size 32 in 0.985 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.145 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.921 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.072 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.945 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.081 sec\n",
            "Step   10000 [1.085 sec/step, loss=2.28794, avg_loss=2.32680]\n",
            "Writing summary at step 10000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/7 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Running evaluation at step 10000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 7/7 [00:11<00:00,  1.66s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saving eval log to /content/logs-SirDavid_first_run/eval-dir..\n",
            "Eval loss for global step 10000: 2.675\n",
            "Writing eval summary!\n",
            "\n",
            "Saving alignment, Mel-Spectrograms and griffin-lim inverted waveform..\n",
            "Input at step 10000: ^so, it is the desert itself that enriches the sea.~____________________________\n",
            "\n",
            "Saving Model Character Embeddings visualization..\n",
            "Tacotron2 Character embeddings have been updated on tensorboard!\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.082 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.916 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.053 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.036 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.092 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.936 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.910 sec\n",
            "Step   10250 [1.090 sec/step, loss=2.34081, avg_loss=2.32150]\n",
            "Writing summary at step 10250\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.232 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.029 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.997 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.998 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.938 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.029 sec\n",
            "Step   10442 [1.075 sec/step, loss=2.08366, avg_loss=2.31049]\n",
            "Generated 32 train batches of size 32 in 0.970 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.128 sec\n",
            "Step   10500 [1.088 sec/step, loss=2.41995, avg_loss=2.30902]\n",
            "Writing summary at step 10500\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.920 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.003 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.905 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.958 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.026 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.902 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.910 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.047 sec\n",
            "Step   10750 [1.087 sec/step, loss=2.34081, avg_loss=2.31078]\n",
            "Writing summary at step 10750\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.005 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.015 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.000 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.996 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.129 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.035 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.862 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.976 sec\n",
            "Step   11000 [1.107 sec/step, loss=2.29405, avg_loss=2.29376]\n",
            "Writing summary at step 11000\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.962 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.985 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.947 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.965 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.256 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.235 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.018 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.989 sec\n",
            "Step   11250 [1.080 sec/step, loss=2.28411, avg_loss=2.28974]\n",
            "Writing summary at step 11250\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.988 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.998 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.926 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.941 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.127 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.014 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.035 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.923 sec\n",
            "Step   11500 [1.104 sec/step, loss=2.34307, avg_loss=2.29077]\n",
            "Writing summary at step 11500\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.068 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.170 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.058 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.103 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.896 sec\n",
            "Step   11685 [1.085 sec/step, loss=2.03598, avg_loss=2.28611]\n",
            "Generated 32 train batches of size 32 in 0.992 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.939 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.185 sec\n",
            "Step   11750 [1.084 sec/step, loss=2.25249, avg_loss=2.27539]\n",
            "Writing summary at step 11750\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.935 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.950 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.924 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.003 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.958 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.081 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.124 sec\n",
            "Step   12000 [1.057 sec/step, loss=2.33658, avg_loss=2.27173]\n",
            "Writing summary at step 12000\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.154 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.123 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.941 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.055 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.975 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.991 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.938 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.062 sec\n",
            "Step   12250 [1.083 sec/step, loss=2.24734, avg_loss=2.27543]\n",
            "Writing summary at step 12250\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.926 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.880 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.011 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.007 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.925 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.032 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.898 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.919 sec\n",
            "Step   12500 [1.088 sec/step, loss=2.16750, avg_loss=2.25840]\n",
            "Writing summary at step 12500\n",
            "\n",
            "Saving alignment, Mel-Spectrograms and griffin-lim inverted waveform..\n",
            "Input at step 12500: ^a colony of leafcutter ants can run into the millions.~_______________\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.966 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.934 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.013 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.876 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.979 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.921 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.118 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.905 sec\n",
            "Step   12750 [1.087 sec/step, loss=2.20308, avg_loss=2.27023]\n",
            "Writing summary at step 12750\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.964 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.948 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.087 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.072 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.966 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.114 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.090 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.898 sec\n",
            "Step   13000 [1.108 sec/step, loss=2.34817, avg_loss=2.25449]\n",
            "Writing summary at step 13000\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.063 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.016 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.956 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.156 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.935 sec\n",
            "Step   13182 [1.071 sec/step, loss=2.16025, avg_loss=2.25217]\n",
            "Generated 32 train batches of size 32 in 1.025 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.053 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.198 sec\n",
            "Step   13250 [1.083 sec/step, loss=2.29519, avg_loss=2.24129]\n",
            "Writing summary at step 13250\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.923 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.974 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.940 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.919 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.909 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.016 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.012 sec\n",
            "Step   13500 [1.094 sec/step, loss=2.27099, avg_loss=2.24803]\n",
            "Writing summary at step 13500\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.976 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.901 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.909 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.082 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.090 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.842 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.877 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.023 sec\n",
            "Step   13750 [1.066 sec/step, loss=2.26465, avg_loss=2.24330]\n",
            "Writing summary at step 13750\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.111 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.809 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.898 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.139 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.060 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.075 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.963 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.047 sec\n",
            "Step   14000 [1.073 sec/step, loss=2.31033, avg_loss=2.24241]\n",
            "Writing summary at step 14000\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.978 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.008 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.037 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.107 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.929 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.935 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.259 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.036 sec\n",
            "Step   14250 [1.131 sec/step, loss=2.37084, avg_loss=2.23186]\n",
            "Writing summary at step 14250\n",
            "Step   14265 [1.147 sec/step, loss=2.15778, avg_loss=2.23100]\n",
            "Generated 32 train batches of size 32 in 1.031 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.110 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.039 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.080 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.244 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.964 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.917 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.033 sec\n",
            "Step   14500 [1.092 sec/step, loss=2.21914, avg_loss=2.22032]\n",
            "Writing summary at step 14500\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.920 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.174 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.155 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.906 sec\n",
            "Step   14648 [1.119 sec/step, loss=1.98247, avg_loss=2.21961]\n",
            "Generated 32 train batches of size 32 in 0.970 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.185 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.023 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.141 sec\n",
            "Step   14750 [1.099 sec/step, loss=2.27555, avg_loss=2.21676]\n",
            "Writing summary at step 14750\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.163 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.114 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.999 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.971 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.968 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.060 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.102 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.003 sec\n",
            "Step   15000 [1.086 sec/step, loss=2.21960, avg_loss=2.21413]\n",
            "Writing summary at step 15000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/7 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Running evaluation at step 15000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 7/7 [00:12<00:00,  1.72s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saving eval log to /content/logs-SirDavid_first_run/eval-dir..\n",
            "Eval loss for global step 15000: 2.713\n",
            "Writing eval summary!\n",
            "\n",
            "Saving alignment, Mel-Spectrograms and griffin-lim inverted waveform..\n",
            "Input at step 15000: ^and its about to get worse.~_____________________\n",
            "\n",
            "Saving Model Character Embeddings visualization..\n",
            "Tacotron2 Character embeddings have been updated on tensorboard!\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.038 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.024 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.869 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.183 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.041 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.931 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.095 sec\n",
            "Step   15250 [1.096 sec/step, loss=2.22144, avg_loss=2.21226]\n",
            "Writing summary at step 15250\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.191 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.922 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.914 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.047 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.008 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.961 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.024 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.038 sec\n",
            "Step   15500 [1.096 sec/step, loss=2.20344, avg_loss=2.20185]\n",
            "Writing summary at step 15500\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.967 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.097 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.157 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.064 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.051 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.958 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.071 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.945 sec\n",
            "Step   15750 [1.100 sec/step, loss=2.11290, avg_loss=2.19891]\n",
            "Writing summary at step 15750\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.029 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.135 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.042 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.961 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.082 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.115 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.050 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.072 sec\n",
            "Step   16000 [1.102 sec/step, loss=2.20050, avg_loss=2.19624]\n",
            "Writing summary at step 16000\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.964 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.028 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.090 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.036 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.026 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.152 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.224 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.065 sec\n",
            "Step   16250 [1.109 sec/step, loss=2.19069, avg_loss=2.19258]\n",
            "Writing summary at step 16250\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.022 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.891 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.071 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.138 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.074 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.907 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.101 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.087 sec\n",
            "Step   16500 [1.126 sec/step, loss=2.06178, avg_loss=2.19104]\n",
            "Writing summary at step 16500\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.158 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.939 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.073 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.056 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.947 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.991 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.902 sec\n",
            "Step   16750 [1.105 sec/step, loss=1.99569, avg_loss=2.18054]\n",
            "Writing summary at step 16750\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.076 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.001 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.090 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.947 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.102 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.991 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.949 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.060 sec\n",
            "Step   17000 [1.088 sec/step, loss=2.03660, avg_loss=2.18114]\n",
            "Writing summary at step 17000\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.131 sec\n",
            "Step   17037 [1.118 sec/step, loss=1.99321, avg_loss=2.18559]\n",
            "Generated 32 train batches of size 32 in 0.967 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.021 sec\n",
            "Step   17101 [1.126 sec/step, loss=1.99098, avg_loss=2.17685]\n",
            "Generated 32 train batches of size 32 in 0.967 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.179 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.892 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.919 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.983 sec\n",
            "Step   17250 [1.091 sec/step, loss=2.10108, avg_loss=2.18003]\n",
            "Writing summary at step 17250\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.890 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.959 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.000 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.992 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.962 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.060 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.084 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.211 sec\n",
            "Step   17500 [1.112 sec/step, loss=1.87172, avg_loss=2.18623]\n",
            "Writing summary at step 17500\n",
            "\n",
            "Saving alignment, Mel-Spectrograms and griffin-lim inverted waveform..\n",
            "Input at step 17500: ^these reefs cover less than one percent of the seafloor,~____________\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.027 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.078 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.046 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.048 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.093 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.166 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.027 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.022 sec\n",
            "Step   17750 [1.093 sec/step, loss=2.06200, avg_loss=2.17508]\n",
            "Writing summary at step 17750\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.122 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.981 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.964 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.051 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.063 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.136 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.001 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.207 sec\n",
            "Step   18000 [1.112 sec/step, loss=2.12782, avg_loss=2.17732]\n",
            "Writing summary at step 18000\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.954 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.970 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.946 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.914 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.108 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.075 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.016 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.030 sec\n",
            "Step   18250 [1.129 sec/step, loss=2.16775, avg_loss=2.16476]\n",
            "Writing summary at step 18250\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.127 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.042 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.016 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.048 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.039 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.073 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.001 sec\n",
            "Step   18500 [1.079 sec/step, loss=2.27994, avg_loss=2.16469]\n",
            "Writing summary at step 18500\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.927 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.973 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.999 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.957 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.121 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.891 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.006 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.038 sec\n",
            "Step   18750 [1.111 sec/step, loss=2.08905, avg_loss=2.15271]\n",
            "Writing summary at step 18750\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.982 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.106 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.173 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.233 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.204 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.934 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.007 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.995 sec\n",
            "Step   19000 [1.113 sec/step, loss=2.23282, avg_loss=2.15320]\n",
            "Writing summary at step 19000\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.981 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.982 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.993 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.056 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.044 sec\n",
            "Step   19172 [1.116 sec/step, loss=1.90403, avg_loss=2.15217]\n",
            "Generated 32 train batches of size 32 in 1.053 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.924 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.946 sec\n",
            "Step   19250 [1.094 sec/step, loss=2.18571, avg_loss=2.15068]\n",
            "Writing summary at step 19250\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.975 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.004 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.014 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.848 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.168 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.117 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.894 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.952 sec\n",
            "Step   19500 [1.116 sec/step, loss=2.25416, avg_loss=2.15442]\n",
            "Writing summary at step 19500\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.000 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.982 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.891 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.042 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.905 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.912 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.973 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.113 sec\n",
            "Step   19750 [1.118 sec/step, loss=2.11543, avg_loss=2.14877]\n",
            "Writing summary at step 19750\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.893 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.045 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.941 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.011 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.079 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.080 sec\n",
            "Step   19969 [1.107 sec/step, loss=1.82285, avg_loss=2.14558]\n",
            "Generated 32 train batches of size 32 in 0.993 sec\n",
            "Step   20000 [1.117 sec/step, loss=2.11128, avg_loss=2.14649]\n",
            "Writing summary at step 20000\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.166 sec\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/7 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Running evaluation at step 20000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 7/7 [00:12<00:00,  1.77s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saving eval log to /content/logs-SirDavid_first_run/eval-dir..\n",
            "Eval loss for global step 20000: 2.737\n",
            "Writing eval summary!\n",
            "\n",
            "Saving alignment, Mel-Spectrograms and griffin-lim inverted waveform..\n",
            "Input at step 20000: ^it is itself an active volcano.~______________________\n",
            "\n",
            "Saving Model Character Embeddings visualization..\n",
            "Tacotron2 Character embeddings have been updated on tensorboard!\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.859 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.052 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.068 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.172 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.018 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.964 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.969 sec\n",
            "Step   20250 [1.122 sec/step, loss=2.31852, avg_loss=2.14315]\n",
            "Writing summary at step 20250\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.023 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.027 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.982 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.053 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.018 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.029 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.061 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.903 sec\n",
            "Step   20500 [1.091 sec/step, loss=2.19850, avg_loss=2.13893]\n",
            "Writing summary at step 20500\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.905 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.090 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.047 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.028 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.012 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.071 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.081 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.840 sec\n",
            "Step   20750 [1.121 sec/step, loss=2.24477, avg_loss=2.13689]\n",
            "Writing summary at step 20750\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.872 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.927 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.135 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.083 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.054 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.202 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.926 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.209 sec\n",
            "Step   21000 [1.122 sec/step, loss=2.06696, avg_loss=2.12731]\n",
            "Writing summary at step 21000\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.018 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.042 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.987 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.191 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.145 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.219 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.078 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.212 sec\n",
            "Step   21250 [1.124 sec/step, loss=2.12337, avg_loss=2.13001]\n",
            "Writing summary at step 21250\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.045 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.140 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.128 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.064 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.926 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.989 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.074 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.109 sec\n",
            "Step   21500 [1.198 sec/step, loss=2.11970, avg_loss=2.13673]\n",
            "Writing summary at step 21500\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.024 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.408 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.106 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.001 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.189 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.031 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.152 sec\n",
            "Step   21750 [1.150 sec/step, loss=2.14717, avg_loss=2.12090]\n",
            "Writing summary at step 21750\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.028 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.079 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.081 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.999 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.054 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.193 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.062 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.263 sec\n",
            "Step   22000 [1.157 sec/step, loss=2.11732, avg_loss=2.11796]\n",
            "Writing summary at step 22000\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.290 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.026 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.038 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.041 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.977 sec\n",
            "Step   22167 [1.179 sec/step, loss=1.94466, avg_loss=2.11669]\n",
            "Generated 32 train batches of size 32 in 1.046 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.992 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.236 sec\n",
            "Step   22250 [1.160 sec/step, loss=2.17587, avg_loss=2.11234]\n",
            "Writing summary at step 22250\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.950 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.101 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.895 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.004 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.106 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.115 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.021 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.032 sec\n",
            "Step   22500 [1.150 sec/step, loss=2.08777, avg_loss=2.12409]\n",
            "Writing summary at step 22500\n",
            "\n",
            "Saving alignment, Mel-Spectrograms and griffin-lim inverted waveform..\n",
            "Input at step 22500: ^theyre very rare.~_____________\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.033 sec\n",
            "Step   22548 [1.142 sec/step, loss=1.90755, avg_loss=2.12092]\n",
            "Generated 32 train batches of size 32 in 1.012 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.174 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.971 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.119 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.011 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.089 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.985 sec\n",
            "Step   22750 [1.171 sec/step, loss=1.95754, avg_loss=2.10571]\n",
            "Writing summary at step 22750\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.058 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.122 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.193 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.112 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.092 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.959 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.995 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.253 sec\n",
            "Step   23000 [1.199 sec/step, loss=2.12490, avg_loss=2.11081]\n",
            "Writing summary at step 23000\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.019 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.112 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.118 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.092 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.033 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.942 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.143 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.908 sec\n",
            "Step   23250 [1.130 sec/step, loss=1.92513, avg_loss=2.10386]\n",
            "Writing summary at step 23250\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.082 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.942 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.884 sec\n",
            "Step   23377 [1.112 sec/step, loss=1.87271, avg_loss=2.10803]\n",
            "Generated 32 train batches of size 32 in 1.006 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.012 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.038 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.076 sec\n",
            "Step   23500 [1.111 sec/step, loss=2.21811, avg_loss=2.10703]\n",
            "Writing summary at step 23500\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.990 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.156 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.065 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.976 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.121 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.000 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.034 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.991 sec\n",
            "Step   23750 [1.114 sec/step, loss=2.09068, avg_loss=2.10436]\n",
            "Writing summary at step 23750\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.980 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.935 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.005 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.134 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.207 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.061 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.081 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.983 sec\n",
            "Step   24000 [1.122 sec/step, loss=2.06217, avg_loss=2.09961]\n",
            "Writing summary at step 24000\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.225 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.022 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.007 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.002 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.938 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.001 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.102 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.100 sec\n",
            "Step   24250 [1.124 sec/step, loss=1.94221, avg_loss=2.10078]\n",
            "Writing summary at step 24250\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.002 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.264 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.082 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.136 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.024 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.079 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.954 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.986 sec\n",
            "Step   24500 [1.165 sec/step, loss=2.07083, avg_loss=2.09318]\n",
            "Writing summary at step 24500\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.160 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.998 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.025 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.862 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.118 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.343 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.975 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.049 sec\n",
            "Step   24750 [1.180 sec/step, loss=1.90761, avg_loss=2.09242]\n",
            "Writing summary at step 24750\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.086 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.901 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.026 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.945 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.917 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.056 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.022 sec\n",
            "Step   25000 [1.143 sec/step, loss=2.19029, avg_loss=2.08959]\n",
            "Writing summary at step 25000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/7 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Running evaluation at step 25000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 7/7 [00:12<00:00,  1.78s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saving eval log to /content/logs-SirDavid_first_run/eval-dir..\n",
            "Eval loss for global step 25000: 2.829\n",
            "Writing eval summary!\n",
            "\n",
            "Saving alignment, Mel-Spectrograms and griffin-lim inverted waveform..\n",
            "Input at step 25000: ^there are more than 7,000 individuals on fernandina alone.~______________________\n",
            "\n",
            "Saving Model Character Embeddings visualization..\n",
            "Tacotron2 Character embeddings have been updated on tensorboard!\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.908 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.040 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.139 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.997 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.075 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.103 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.926 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.110 sec\n",
            "Step   25250 [1.156 sec/step, loss=2.22331, avg_loss=2.09119]\n",
            "Writing summary at step 25250\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.071 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.081 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.132 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.964 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.142 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.205 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.093 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.095 sec\n",
            "Step   25500 [1.198 sec/step, loss=2.07062, avg_loss=2.07957]\n",
            "Writing summary at step 25500\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.024 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.273 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.081 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.059 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.854 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.933 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.138 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.211 sec\n",
            "Step   25750 [1.176 sec/step, loss=2.04302, avg_loss=2.08997]\n",
            "Writing summary at step 25750\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.012 sec\n",
            "Step   25798 [1.197 sec/step, loss=2.01349, avg_loss=2.08943]\n",
            "Generated 32 train batches of size 32 in 1.082 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.036 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.938 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.092 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.136 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.912 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.215 sec\n",
            "Step   26000 [1.172 sec/step, loss=2.08542, avg_loss=2.08733]\n",
            "Writing summary at step 26000\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.050 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.998 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.214 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.062 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.913 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.947 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.953 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.985 sec\n",
            "Step   26250 [1.118 sec/step, loss=2.06027, avg_loss=2.07915]\n",
            "Writing summary at step 26250\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.052 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.050 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.037 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.008 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.072 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.824 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.964 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.892 sec\n",
            "Step   26500 [1.149 sec/step, loss=2.09739, avg_loss=2.07661]\n",
            "Writing summary at step 26500\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.996 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.910 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.987 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.093 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.883 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.821 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.327 sec\n",
            "Step   26750 [1.120 sec/step, loss=2.12520, avg_loss=2.07803]\n",
            "Writing summary at step 26750\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.999 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.959 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.945 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.179 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.174 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.063 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.064 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.002 sec\n",
            "Step   27000 [1.124 sec/step, loss=2.11620, avg_loss=2.07729]\n",
            "Writing summary at step 27000\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.984 sec\n",
            "Step   27041 [1.131 sec/step, loss=1.81931, avg_loss=2.07191]\n",
            "Generated 32 train batches of size 32 in 1.064 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.147 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.007 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.050 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.001 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.153 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.105 sec\n",
            "Step   27250 [1.156 sec/step, loss=2.01585, avg_loss=2.06482]\n",
            "Writing summary at step 27250\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.050 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.982 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.973 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.946 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.876 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.986 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.201 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.907 sec\n",
            "Step   27500 [1.154 sec/step, loss=1.98130, avg_loss=2.06665]\n",
            "Writing summary at step 27500\n",
            "\n",
            "Saving alignment, Mel-Spectrograms and griffin-lim inverted waveform..\n",
            "Input at step 27500: ^and theyre now almost as surefooted as their parents.~___________\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.104 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.929 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.177 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.012 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.897 sec\n",
            "Step   27678 [1.155 sec/step, loss=1.74456, avg_loss=2.06395]\n",
            "Generated 32 train batches of size 32 in 0.912 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.051 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.051 sec\n",
            "Step   27750 [1.154 sec/step, loss=2.12460, avg_loss=2.06513]\n",
            "Writing summary at step 27750\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.200 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.152 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.103 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.040 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.973 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.100 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.079 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.989 sec\n",
            "Step   28000 [1.174 sec/step, loss=2.10592, avg_loss=2.07777]\n",
            "Writing summary at step 28000\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.892 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.214 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.974 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.945 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.215 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.087 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.972 sec\n",
            "Step   28250 [1.145 sec/step, loss=2.12677, avg_loss=2.05920]\n",
            "Writing summary at step 28250\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.205 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.099 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.885 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.960 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.980 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.000 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.056 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.039 sec\n",
            "Step   28500 [1.165 sec/step, loss=1.92493, avg_loss=2.06458]\n",
            "Writing summary at step 28500\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.189 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.077 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.167 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.961 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.130 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.934 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.967 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.014 sec\n",
            "Step   28750 [1.123 sec/step, loss=2.14540, avg_loss=2.06709]\n",
            "Writing summary at step 28750\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.153 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.151 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.080 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.123 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.976 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.900 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.012 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.870 sec\n",
            "Step   29000 [1.089 sec/step, loss=2.02401, avg_loss=2.05630]\n",
            "Writing summary at step 29000\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.932 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.061 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.042 sec\n",
            "Step   29112 [1.107 sec/step, loss=1.95490, avg_loss=2.05714]\n",
            "Generated 32 train batches of size 32 in 1.012 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.988 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.033 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.997 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.082 sec\n",
            "Step   29250 [1.099 sec/step, loss=2.07479, avg_loss=2.05687]\n",
            "Writing summary at step 29250\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.082 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.038 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 1.132 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.922 sec\n",
            "\n",
            "Generated 32 train batches of size 32 in 0.942 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G52PCeEfqG_S",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate\n",
        "\n",
        "Synthesize/Evaluate the Tacotron model. Gives the synth_output folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEgYjKjR9INq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}